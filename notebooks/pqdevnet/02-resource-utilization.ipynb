{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource Utilization\n",
    "\n",
    "CPU, memory, disk I/O, and network throughput analysis for PQ Devnet clients.\n",
    "\n",
    "This notebook examines container-level resource usage using cAdvisor metrics:\n",
    "- CPU usage (cores) per client\n",
    "- Memory working set and RSS per client\n",
    "- Disk read/write throughput and usage\n",
    "- Network receive/transmit throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters - injected by papermill\n",
    "devnet_id = None  # e.g., \"devnet-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Set default renderer for static HTML output\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve devnet_id\n",
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "if devnet_id is None:\n",
    "    # Use latest devnet from manifest\n",
    "    devnets_path = DATA_DIR / \"devnets.json\"\n",
    "    if devnets_path.exists():\n",
    "        with open(devnets_path) as f:\n",
    "            devnets = json.load(f).get(\"devnets\", [])\n",
    "        if devnets:\n",
    "            devnet_id = devnets[-1][\"id\"]  # Latest\n",
    "            print(f\"Using latest devnet: {devnet_id}\")\n",
    "    else:\n",
    "        raise ValueError(\"No devnets.json found. Run 'just detect-devnets' first.\")\n",
    "\n",
    "DEVNET_DIR = DATA_DIR / devnet_id\n",
    "print(f\"Loading data from: {DEVNET_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load devnet metadata\n",
    "with open(DATA_DIR / \"devnets.json\") as f:\n",
    "    devnets_data = json.load(f)\n",
    "    devnet_info = next((d for d in devnets_data[\"devnets\"] if d[\"id\"] == devnet_id), None)\n",
    "\n",
    "if devnet_info:\n",
    "    print(f\"Devnet: {devnet_info['id']}\")\n",
    "    print(f\"Duration: {devnet_info['duration_hours']:.1f} hours\")\n",
    "    print(f\"Time: {devnet_info['start_time']} to {devnet_info['end_time']}\")\n",
    "    print(f\"Slots: {devnet_info['start_slot']} \\u2192 {devnet_info['end_slot']}\")\n",
    "    print(f\"Clients: {', '.join(devnet_info['clients'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_bytes(val: float) -> str:\n",
    "    \"\"\"Format bytes to human-readable units.\"\"\"\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if abs(val) < 1024:\n",
    "            return f\"{val:.1f} {unit}\"\n",
    "        val /= 1024\n",
    "    return f\"{val:.1f} PB\"\n",
    "\n",
    "\n",
    "def format_bytes_per_sec(val: float) -> str:\n",
    "    \"\"\"Format bytes/s to human-readable units.\"\"\"\n",
    "    return format_bytes(val) + \"/s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load container resource data\n",
    "data_files = {\n",
    "    \"cpu\": \"container_cpu.parquet\",\n",
    "    \"memory\": \"container_memory.parquet\",\n",
    "    \"disk_io\": \"container_disk_io.parquet\",\n",
    "    \"network\": \"container_network.parquet\",\n",
    "}\n",
    "\n",
    "# Infrastructure containers irrelevant to devnet client analysis\n",
    "EXCLUDED_CONTAINERS = {\"unknown\", \"cadvisor\", \"prometheus\", \"promtail\", \"node-exporter\", \"node_exporter\", \"grafana\"}\n",
    "\n",
    "# Aggregation strategy per data type:\n",
    "# - cpu/memory: max (gauge-like, take the active container's value)\n",
    "# - disk_io/network: sum (per-device/interface rates should be summed)\n",
    "AGG_STRATEGY = {\"cpu\": \"max\", \"memory\": \"max\", \"disk_io\": \"sum\", \"network\": \"sum\"}\n",
    "\n",
    "# Group-by columns per data type (all have container+timestamp, some have metric)\n",
    "GROUP_COLS = {\n",
    "    \"cpu\": [\"container\", \"timestamp\"],\n",
    "    \"memory\": [\"container\", \"metric\", \"timestamp\"],\n",
    "    \"disk_io\": [\"container\", \"metric\", \"timestamp\"],\n",
    "    \"network\": [\"container\", \"metric\", \"timestamp\"],\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "for key, filename in data_files.items():\n",
    "    path = DEVNET_DIR / filename\n",
    "    if path.exists():\n",
    "        df = pd.read_parquet(path)\n",
    "        df = df[~df[\"container\"].isin(EXCLUDED_CONTAINERS)]\n",
    "        # Deduplicate: multiple Prometheus series (interfaces, devices, container\n",
    "        # IDs after restarts) can produce duplicate rows per container+timestamp.\n",
    "        df = df.groupby(GROUP_COLS[key], as_index=False)[\"value\"].agg(AGG_STRATEGY[key])\n",
    "        dfs[key] = df\n",
    "        print(f\"{key}: {len(df)} records, containers: {df['container'].nunique()}\")\n",
    "    else:\n",
    "        dfs[key] = pd.DataFrame()\n",
    "        print(f\"{key}: no data (file not found)\")\n",
    "\n",
    "# Unified container list for consistent chart layout\n",
    "all_containers = sorted(set().union(*(df[\"container\"].unique() for df in dfs.values() if not df.empty)))\n",
    "n_cols = min(len(all_containers), 2)\n",
    "n_rows = -(-len(all_containers) // n_cols)\n",
    "print(f\"\\nAll containers ({len(all_containers)}): {all_containers}\")\n",
    "\n",
    "# Overall time range for empty chart placeholders\n",
    "all_ts = pd.concat([df[\"timestamp\"] for df in dfs.values() if not df.empty])\n",
    "t_min, t_max = all_ts.min(), all_ts.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Usage\n",
    "\n",
    "CPU cores used per container over time, derived from `rate(container_cpu_usage_seconds_total[5m])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_df = dfs[\"cpu\"]\n",
    "\n",
    "if cpu_df.empty:\n",
    "    print(\"No CPU data available\")\n",
    "else:\n",
    "    fig = make_subplots(\n",
    "        rows=n_rows, cols=n_cols,\n",
    "        subplot_titles=all_containers,\n",
    "        vertical_spacing=0.12 / max(n_rows - 1, 1) * 2,\n",
    "        horizontal_spacing=0.08,\n",
    "    )\n",
    "\n",
    "    for i, container in enumerate(all_containers):\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "        cdf = cpu_df[cpu_df[\"container\"] == container].sort_values(\"timestamp\")\n",
    "        if not cdf.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=cdf[\"timestamp\"], y=cdf[\"value\"],\n",
    "                    name=container, showlegend=False,\n",
    "                    line=dict(color=\"#636EFA\"),\n",
    "                ),\n",
    "                row=row, col=col,\n",
    "            )\n",
    "        else:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[t_min, t_max], y=[0, 0], name=container, showlegend=False, line=dict(color=\"#636EFA\")),\n",
    "                row=row, col=col,\n",
    "            )\n",
    "        fig.update_yaxes(title_text=\"CPU (cores)\", row=row, col=col)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"CPU Usage per Container\",\n",
    "        height=270 * n_rows,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU summary statistics\n",
    "if not cpu_df.empty:\n",
    "    cpu_summary = cpu_df.groupby(\"container\")[\"value\"].agg(\n",
    "        [\"mean\", \"max\", \"min\", \"std\"]\n",
    "    ).round(3)\n",
    "    cpu_summary.columns = [\"Mean (cores)\", \"Max (cores)\", \"Min (cores)\", \"Std Dev\"]\n",
    "    cpu_summary = cpu_summary.sort_index()\n",
    "    display(cpu_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage\n",
    "\n",
    "Memory consumption per container, including working set (total usage minus inactive file cache) and RSS (Resident Set Size -- anonymous memory only, excluding file-backed pages). The gap between the two shows active file cache usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_df = dfs[\"memory\"]\n",
    "\n",
    "if mem_df.empty:\n",
    "    print(\"No memory data available\")\n",
    "else:\n",
    "    # Combine working_set and rss for per-container comparison\n",
    "    mem_plot_df = mem_df[mem_df[\"metric\"].isin([\"working_set\", \"rss\"])].copy()\n",
    "    if not mem_plot_df.empty:\n",
    "        mem_plot_df[\"value_mb\"] = mem_plot_df[\"value\"] / (1024 * 1024)\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows, cols=n_cols,\n",
    "            subplot_titles=all_containers,\n",
    "            vertical_spacing=0.12 / max(n_rows - 1, 1) * 2,\n",
    "            horizontal_spacing=0.08,\n",
    "        )\n",
    "\n",
    "        colors = {\"working_set\": \"#636EFA\", \"rss\": \"#EF553B\"}\n",
    "        legend_added = set()\n",
    "\n",
    "        for i, container in enumerate(all_containers):\n",
    "            row = i // n_cols + 1\n",
    "            col = i % n_cols + 1\n",
    "            cdf = mem_plot_df[mem_plot_df[\"container\"] == container]\n",
    "            if cdf.empty:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=[t_min, t_max], y=[0, 0], name=container, showlegend=False, line=dict(color=\"#636EFA\")),\n",
    "                    row=row, col=col,\n",
    "                )\n",
    "            else:\n",
    "                for metric in [\"working_set\", \"rss\"]:\n",
    "                    mdf = cdf[cdf[\"metric\"] == metric].sort_values(\"timestamp\")\n",
    "                    if mdf.empty:\n",
    "                        continue\n",
    "                    show_legend = metric not in legend_added\n",
    "                    legend_added.add(metric)\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=mdf[\"timestamp\"], y=mdf[\"value_mb\"],\n",
    "                            name=metric, legendgroup=metric,\n",
    "                            showlegend=show_legend,\n",
    "                            line=dict(color=colors[metric]),\n",
    "                        ),\n",
    "                        row=row, col=col,\n",
    "                    )\n",
    "            fig.update_yaxes(title_text=\"MB\", row=row, col=col)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=\"Memory Usage per Container (Working Set vs RSS)\",\n",
    "            height=270 * n_rows,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory summary\n",
    "if not mem_df.empty:\n",
    "    ws_df = mem_df[mem_df[\"metric\"] == \"working_set\"]\n",
    "    if not ws_df.empty:\n",
    "        mem_summary = ws_df.groupby(\"container\")[\"value\"].agg([\"mean\", \"max\"]).reset_index()\n",
    "        mem_summary[\"Mean\"] = mem_summary[\"mean\"].apply(format_bytes)\n",
    "        mem_summary[\"Peak\"] = mem_summary[\"max\"].apply(format_bytes)\n",
    "        mem_summary = mem_summary.rename(columns={\"container\": \"Container\"})[[\"Container\", \"Mean\", \"Peak\"]]\n",
    "        mem_summary = mem_summary.sort_values(\"Container\")\n",
    "        display(mem_summary.set_index(\"Container\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disk I/O\n",
    "\n",
    "Disk read/write throughput and total disk usage per container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_df = dfs[\"disk_io\"]\n",
    "\n",
    "if disk_df.empty:\n",
    "    print(\"No disk I/O data available\")\n",
    "else:\n",
    "    # Read/write throughput per container\n",
    "    throughput_df = disk_df[disk_df[\"metric\"].isin([\"read_throughput\", \"write_throughput\"])].copy()\n",
    "    if not throughput_df.empty:\n",
    "        throughput_df[\"value_mb\"] = throughput_df[\"value\"] / (1024 * 1024)\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows, cols=n_cols,\n",
    "            subplot_titles=all_containers,\n",
    "            vertical_spacing=0.12 / max(n_rows - 1, 1) * 2,\n",
    "            horizontal_spacing=0.08,\n",
    "        )\n",
    "\n",
    "        colors = {\"read_throughput\": \"#636EFA\", \"write_throughput\": \"#EF553B\"}\n",
    "        legend_added = set()\n",
    "\n",
    "        for i, container in enumerate(all_containers):\n",
    "            row = i // n_cols + 1\n",
    "            col = i % n_cols + 1\n",
    "            cdf = throughput_df[throughput_df[\"container\"] == container]\n",
    "            if cdf.empty:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=[t_min, t_max], y=[0, 0], name=container, showlegend=False, line=dict(color=\"#636EFA\")),\n",
    "                    row=row, col=col,\n",
    "                )\n",
    "            else:\n",
    "                for metric in [\"read_throughput\", \"write_throughput\"]:\n",
    "                    mdf = cdf[cdf[\"metric\"] == metric].sort_values(\"timestamp\")\n",
    "                    if mdf.empty:\n",
    "                        continue\n",
    "                    label = metric.replace(\"_throughput\", \"\")\n",
    "                    show_legend = metric not in legend_added\n",
    "                    legend_added.add(metric)\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=mdf[\"timestamp\"], y=mdf[\"value_mb\"],\n",
    "                            name=label, legendgroup=metric,\n",
    "                            showlegend=show_legend,\n",
    "                            line=dict(color=colors[metric]),\n",
    "                        ),\n",
    "                        row=row, col=col,\n",
    "                    )\n",
    "            fig.update_yaxes(title_text=\"MB/s\", row=row, col=col)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=\"Disk I/O Throughput per Container (Read vs Write)\",\n",
    "            height=270 * n_rows,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disk usage over time per container\n",
    "if not disk_df.empty:\n",
    "    usage_df = disk_df[disk_df[\"metric\"] == \"disk_usage\"].copy()\n",
    "    if not usage_df.empty:\n",
    "        usage_df[\"value_gb\"] = usage_df[\"value\"] / (1024 * 1024 * 1024)\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows, cols=n_cols,\n",
    "            subplot_titles=all_containers,\n",
    "            vertical_spacing=0.12 / max(n_rows - 1, 1) * 2,\n",
    "            horizontal_spacing=0.08,\n",
    "        )\n",
    "\n",
    "        for i, container in enumerate(all_containers):\n",
    "            row = i // n_cols + 1\n",
    "            col = i % n_cols + 1\n",
    "            cdf = usage_df[usage_df[\"container\"] == container].sort_values(\"timestamp\")\n",
    "            if not cdf.empty:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=cdf[\"timestamp\"], y=cdf[\"value_gb\"],\n",
    "                        name=container, showlegend=False,\n",
    "                        line=dict(color=\"#636EFA\"),\n",
    "                    ),\n",
    "                    row=row, col=col,\n",
    "                )\n",
    "            else:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=[t_min, t_max], y=[0, 0], name=container, showlegend=False, line=dict(color=\"#636EFA\")),\n",
    "                    row=row, col=col,\n",
    "                )\n",
    "            fig.update_yaxes(title_text=\"GB\", row=row, col=col)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=\"Disk Usage per Container\",\n",
    "            height=270 * n_rows,\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Throughput\n",
    "\n",
    "Network receive (rx) and transmit (tx) throughput per container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_df = dfs[\"network\"]\n",
    "\n",
    "if net_df.empty:\n",
    "    print(\"No network data available\")\n",
    "else:\n",
    "    net_df[\"value_mb\"] = net_df[\"value\"] / (1024 * 1024)\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=n_rows, cols=n_cols,\n",
    "        subplot_titles=all_containers,\n",
    "        vertical_spacing=0.12 / max(n_rows - 1, 1) * 2,\n",
    "        horizontal_spacing=0.08,\n",
    "    )\n",
    "\n",
    "    colors = {\"rx\": \"#636EFA\", \"tx\": \"#EF553B\"}\n",
    "    legend_added = set()\n",
    "\n",
    "    for i, container in enumerate(all_containers):\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "        cdf = net_df[net_df[\"container\"] == container]\n",
    "        if cdf.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[t_min, t_max], y=[0, 0], name=container, showlegend=False, line=dict(color=\"#636EFA\")),\n",
    "                row=row, col=col,\n",
    "            )\n",
    "        else:\n",
    "            for metric in [\"rx\", \"tx\"]:\n",
    "                mdf = cdf[cdf[\"metric\"] == metric].sort_values(\"timestamp\")\n",
    "                if mdf.empty:\n",
    "                    continue\n",
    "                show_legend = metric not in legend_added\n",
    "                legend_added.add(metric)\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=mdf[\"timestamp\"], y=mdf[\"value_mb\"],\n",
    "                        name=metric, legendgroup=metric,\n",
    "                        showlegend=show_legend,\n",
    "                        line=dict(color=colors[metric]),\n",
    "                    ),\n",
    "                    row=row, col=col,\n",
    "                )\n",
    "        fig.update_yaxes(title_text=\"MB/s\", row=row, col=col)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Network Throughput per Container (RX vs TX)\",\n",
    "        height=270 * n_rows,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Peak and average resource usage per container across the devnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary table across all resource types\n",
    "summary_rows = []\n",
    "\n",
    "# CPU\n",
    "if not cpu_df.empty:\n",
    "    for container, group in cpu_df.groupby(\"container\"):\n",
    "        summary_rows.append({\n",
    "            \"Container\": container,\n",
    "            \"Avg CPU (cores)\": f\"{group['value'].mean():.3f}\",\n",
    "            \"Peak CPU (cores)\": f\"{group['value'].max():.3f}\",\n",
    "        })\n",
    "\n",
    "# Memory\n",
    "if not mem_df.empty:\n",
    "    ws_df = mem_df[mem_df[\"metric\"] == \"working_set\"]\n",
    "    for container, group in ws_df.groupby(\"container\"):\n",
    "        existing = next((r for r in summary_rows if r[\"Container\"] == container), None)\n",
    "        if existing is None:\n",
    "            existing = {\"Container\": container}\n",
    "            summary_rows.append(existing)\n",
    "        existing[\"Avg Memory\"] = format_bytes(group[\"value\"].mean())\n",
    "        existing[\"Peak Memory\"] = format_bytes(group[\"value\"].max())\n",
    "\n",
    "# Network\n",
    "if not net_df.empty:\n",
    "    for container, group in net_df.groupby(\"container\"):\n",
    "        existing = next((r for r in summary_rows if r[\"Container\"] == container), None)\n",
    "        if existing is None:\n",
    "            existing = {\"Container\": container}\n",
    "            summary_rows.append(existing)\n",
    "        rx = group[group[\"metric\"] == \"rx\"][\"value\"]\n",
    "        tx = group[group[\"metric\"] == \"tx\"][\"value\"]\n",
    "        if not rx.empty:\n",
    "            existing[\"Avg RX\"] = format_bytes_per_sec(rx.mean())\n",
    "        if not tx.empty:\n",
    "            existing[\"Avg TX\"] = format_bytes_per_sec(tx.mean())\n",
    "\n",
    "if summary_rows:\n",
    "    summary_df = pd.DataFrame(summary_rows).set_index(\"Container\").sort_index().fillna(\"-\")\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"No resource data available for summary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Devnet: {devnet_id}\")\n",
    "if devnet_info:\n",
    "    print(f\"Duration: {devnet_info['duration_hours']:.1f} hours\")\n",
    "print(f\"Containers analyzed: {cpu_df['container'].nunique() if not cpu_df.empty else 0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
